<instructions>
FOLLOW THIS EXACT STRUCTURE. Output sections in order 1-6.

1. **Header**: Sprint ID, Goal, Dates, Status, Delivery
2. **Â§1 What Was Delivered**: User-facing summary â€” what's accessible/usable vs what's internal/backend
3. **Â§2 Story Results**: Table of all stories with final status and per-story metrics
4. **Â§3 Execution Metrics**: AI performance metrics â€” tokens, duration, bounces, correction tax
5. **Â§4 Lessons Learned**: Flagged from agent reports, pending user approval to record
6. **Â§5 Retrospective**: What went well, what didn't, and what to change â€” covers both project and delivery process

This template is used by the Team Lead at Sprint Consolidation (Step 7).
Written to `.bounce/sprint-report.md`, then archived to `.bounce/archive/S-{XX}/`.

When the delivery completes, sprint reports are summarized into the Roadmap Â§7 Delivery Log
as Release Notes.

The Â§1 section is critical â€” the user wants to know what they can see, touch, and use NOW
versus what was built under the hood. Write it from the user's perspective, not the developer's.

Do NOT output these instructions.
</instructions>

# Sprint Report: S-{XX}

---

> **Sprint Goal**: {One-sentence North Star}
> **Dates**: {MM/DD - MM/DD}
> **Status**: Achieved / Partially Achieved / Failed
> **Delivery**: D-{NN}_{release_name}
> **Delivery Plan**: `product_plans/{delivery}/DELIVERY_PLAN.md`

---

## 1. What Was Delivered

### User-Facing (Accessible Now)
> Features, screens, or workflows the user can see, interact with, or test right now.

- {e.g., "Login page with email/password authentication â€” accessible at /login"}
- {e.g., "Dashboard with project overview cards â€” accessible at /dashboard"}
- {e.g., "Settings page with profile editing â€” accessible at /settings"}

### Internal / Backend (Not Directly Visible)
> Infrastructure, APIs, database changes, or plumbing that supports future features.

- {e.g., "User roles and permissions system â€” API endpoints ready, no UI yet"}
- {e.g., "Database migration for notifications table â€” schema deployed"}
- {e.g., "Rate limiting middleware on all API routes"}

### Not Completed
> Stories that were planned but didn't make it (escalated, deferred, or partially done).

- {e.g., "STORY-001-03: Email notifications â€” Escalated (template integration failed 3x)"}

### Product Docs Affected
> Any product documentation files modified, created, or identified for updates. Handed off to Scribe agent.

- {e.g., "product_documentation/api_reference.md â€” Added rate limiting details"}

---

## 2. Story Results

| Story | Epic | Label | Final State | Bounces (QA) | Bounces (Arch) | Correction Tax |
|-------|------|-------|-------------|--------------|----------------|----------------|
| STORY-{ID}: {name} | EPIC-{ID} | L{N} | Done / Escalated / Parking Lot | {N} | {N} | {X}% |

### Story Highlights
- **{STORY-ID}**: {1-sentence summary of what was built and any notable decisions}

### Escalated Stories (if any)
- **{STORY-ID}**: Escalated after {N} bounces. Root cause: {why}. Recommendation: {rewrite spec / descope / kill}.

---

## 3. Execution Metrics

### AI Performance

| Metric | Value | Notes |
|--------|-------|-------|
| **Total Tokens Used** | {N} | input + output across all agents |
| **Output Tokens** | {N} | tokens generated by agents |
| **Input Tokens** | {N} | context fed to agents |
| **Total Execution Duration** | {X}h {Y}m | wall-clock time from sprint start to completion |
| **Agent Sessions** | {N} | total subagent spawns (Dev + QA + Arch + DevOps + Scribe) |
| **Estimated Cost** | ${X.XX} | based on model pricing at time of execution |

### V-Bounce Quality

| Metric | Value | Notes |
|--------|-------|-------|
| **Stories Planned** | {X} | |
| **Stories Delivered** | {Y} | |
| **Stories Escalated** | {Z} | |
| **Total QA Bounces** | {N} | across all stories |
| **Total Architect Bounces** | {N} | across all stories |
| **Bounce Ratio** | {X}% | (total bounces / total stories) |
| **Average Correction Tax** | {X}% | (0% = autonomous, 100% = human rewrote everything) |
| **First-Pass Success Rate** | {X}% | stories that passed QA on first try |
| **Merge Conflicts** | {N} simple, {N} complex | |

### Per-Story Breakdown

| Story | Tokens | Duration | Agent Sessions | Bounces | Cost |
|-------|--------|----------|----------------|---------|------|
| STORY-{ID} | {N} | {Xh Ym} | {N} | {N} | ${X.XX} |

---

## 4. Lessons Learned

> Flagged by agents during the sprint. Each needs user approval before recording to LESSONS.md.

| Source | Lesson | Approved? |
|--------|--------|-----------|
| STORY-{ID} Dev Report | {What happened and proposed rule} | Pending / Yes / No |

---

## 5. Retrospective

### What Went Well
> Practices, patterns, or decisions that worked and should be repeated.

- {e.g., "Splitting EPIC-001 into small L2 stories kept bounces low â€” 80% first-pass success"}
- {e.g., "Reading _manifest.json upfront saved Dev from duplicating existing API docs"}

### What Didn't Go Well
> Problems encountered, root causes, and impact on the sprint.

- {e.g., "STORY-001-03 escalated because the spec was ambiguous â€” 3 QA bounces wasted"}
- {e.g., "Merge conflict on STORY-002-01 required a fix story â€” cost an extra half-day"}

### Process Improvements
> Changes to the V-Bounce delivery process itself â€” not just this project, but how we work.

- {e.g., "Story specs for L3+ stories should require Architect pre-review before bouncing"}
- {e.g., "QA found the same import ordering issue in 3 stories â€” add linting rule to Dev setup, not just LESSONS.md"}
- {e.g., "L3 stories consumed 4x the tokens of L2s â€” add a complexity budget to sprint planning"}
- {e.g., "First-pass success rate was 40% â€” require Ambiguity ðŸŸ¢ on all stories, not just Context Pack checks"}
- {e.g., "Sprint Integration Audit (Step 6) found no issues in 3 consecutive sprints â€” consider making it conditional for small sprints"}

---

## 6. Change Log

| Date | Change | By |
|------|--------|-----|
| {YYYY-MM-DD} | Sprint Report generated | Team Lead |
